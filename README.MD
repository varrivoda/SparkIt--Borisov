# Часть 1, Введение, описание задачи, настройка проекта

1:30
После как разберемся, как в принципе это работает, первая конкретная задача - это зарегистрировать бины и наших репозитории, из классов, которых в принципе не существует природе. Как работает Спринг-Дата? Мы пишем какой-то там интерфейс, там методы findByName() и т.д., класс не пишем. И потом во все наши сервисы инжектить, все отлично инжектится. При этом если посмотреть реализацию того что занжектилось, то там все-таки какой-то класс есть, соответственно нам надо разобраться, как можно из несуществующих классов насоздавать сингальтонов и засунуть их в контекст.

Есть разные техники, как этому как как этому можно прийти, и мы обсудим, что нам более удобно - это можно делать через регистрары (см предыдущий доклад), это можно делать через ApplicationContext6Initializer'ы (см доклад про СпрингБут с кириллом). Можно это попытаться через Листенер сделать. Нам надо понять куда мы воткнём вот этот код, который сделает эту магию: нагенерит классов, насоздает бинов, и зарегистрирует их в контексте.

Дальше у нас будет очень интересная дилемма потому что как вы понимаете когда пишется какой-то фреймворк на Спринге, который должен что-то делать с этим с нашим контекстом, туда чё-то регистрировать то мы не сможем пользоваться спрингом на этапе разработки нашего кода, то есть когда вы берете спринг и начинате писать бизнес-логику, вы можете там autoiwired делать, constructor injection, transaction и всю прочую магию. А когда вы пишете какой-то код, который будет что-то совать в контекст, на этапе когда контекст еще не построен, то как бы не получится использовать Спринг, а поскольку писать мы будем сложную штуку, то будет интересно написать так чтобы в конечном итоге код можно было поддерживать-читать, чтобы все было понятно.

3:26
Заодно рассмотрим, как в Hibernate реализованная загрузка ленивых коллекций. Это немножко выходит за рамки про СпрингДату, но это тем не менее очень связано со всем этим миром (см доклад Н. Алименкова "босиком по граблям Hibernate", там он рассказывает, какие неправильные ответы ему дают на собеседованиях, а как на самом деле это реализовано, так и не сказал, мы здесь откроем это секрет) Наш аналог СпрингДаты будет возвращать объекты, у которых могут быть вложены ленивые коллекции, который был подгружаться тогда как кним будут обращаться.

4:45 Слайд 1, ПЛАН ДОКЛАДА
1) Как написать (регистрация бинов из несуществующих классов)
2) Где написать (Regiatrar? AppCtxInitializer? ContextRefresh?)
3) Какнаписать красивый код (без Спринга)
4) Раскрыть секрет Алименкова (ленивые коллекции)
5) Как это сделано

последний пункт на слайде - посмотреть как это в принципе все сделано в реальной СпрингДате.
При подготовке доклада возникла дилема, сразу залезть посмотреть в кишки СпроингаДаты, или написать что-то аналогичное не подглядывая, а потом сравнить. И решил вот пойти по этому пути.

Когда я закончил и расковырял СпрингДату (спасибо Г Зайцеву из Epamm), то при сравнении с тем что я написал, нашлись вещи которые принципиально отличаются, но самые важные вещи оказались достаточно похожи, поэтому если мы напишем полностью с нуля, нам будет намного более понятно как это все работает, чем если мы будем смотреть дикое количество очень сложных классов которые не везде написаны по дизайн-паттернам. Видимо когда пишут такие сложные фреймворки, редко получается написать идеально. Встречаются методы которые делают сразу 3 вещи одновременно, по двести-триста строчек кода внутри метода. Очень сложно это понимать, намного проще если мы напишем это все с нуля.

Итак, начнинаем думать, _что_ мы будем писать.
В конвенции спрингДата мы должны создавать интерфейс, у которого будут методы, которые вот там типа fineBy...(), и если задуматься, а что можно сообщить при помощи названия этого метода то тут есть как бы по большому счету на данный момент два вида операции: у нас есть разные виды фильтров потому что после findBy..() при помощи and мы можем клеить
разные фильтры. типатакого
findByNameContainsSortByAge(String partOfName)

у фильтра есть название у фильтра есть field по которым
фильтруется это может быть контент может быть и просто может быть там целый список слов и можно добавлять сортировки на какие-то полям и приятно при помощи and клеить какие филды должны участвовать в сортировке.

поскольку мы будем писать аналог СспрингДаты для spark там на самом деле возможно
мы захотим чуть чуть больше возможности всяких разных делать поэтому мы задумались об этом чуть ближе к делу

7:35
чтобы все это заработало, в первую очередь нам надо написать какой-то код, который на каждый такой интерфейс будет генерить динамический класс на лету. Вообще чтобы сгенерить класс на лету у нас есть разные библиотеки, которые умеют это делать. мы будем пользоваться стандартной Java-библиотекой которая называется Дайнамик прокси, она идет джаве в пакете java.lang.reflect - не надо будет импортировать, потому что такая достаточно стандартная штука, очень многие инфраструктуры и пользуются потому что очень многие фромборке генерят на лету классы. Есть еще одна порулярная библиотека, которая тоже умеет генерить классы, она называется CGLib, и она действует немножко по другому: она создает наследника от какого-то указанного класса и например Мокито пользуются для ваших моков, Hibernate пользуется когда он там тоже делать всякие разные прокси для ваших entity...  Нам эта библиотека не понадобится, потому что мы хотим генерить классы из интерфейсов. Библиотека Дайнамик прокси умеет создавать класс который реализует указанный интерфейс или несколько интерфейсов и понятно что в сгенерированном классе будут все методы, которые указаны в интерфейсе. Вопрос - что эти методы будут делать? Ну понятно что можно написать какой-то код который будет создавать байт-код и прописывать какие-то методы, но как он догадается что эти методы должны делать?

В дизайн паттерне Прокси это все выглядит следующим образом: вначале вы пишете интерфейс, потом вы создаете прокси класс, в котором будут те же самые методы. Реализации этих методов идет в InvocationHandler, который вы при создании прокси класса передаете, у него один-единственный метод invoke(), поэтому можно его даже лямбда передать, и он принимает сигнатуру метода который вызвали его прокси-класс,  аргументы которые ему передали и зачем-то передается сам объект прокси.

Визуально я долго думал какую аллегорию взять мне кажется что очень хорошо олицетворяет дизайн паттерн Прокси вот этот Крэнг из Черепашек Ниндзя: допустим, эта штука будет не прозрачна и люди которые будут общаться с этим существом, они будут думать что это он, соответственно ему вызывают какие-то методы, например беги(30). Он не знает что делать, но у него есть InvocationHandler, который сидит внутри, потому что мы передадим tuij при создании этогопроски, и он будет там прикреплен и соответственно каждый раз на вызов любого метода он будет бежать к своему InvocationHandler'у -
"тут меня вызвали метод бежать и параметр 30 километров в час, что надо делать?"
И в результате логику по факту будет реализовывать InvocationHandler, если метод беги что-то
возвращает то соответственно опять же наш хэндлер должен будет что-то вернуть обратно прокси, прокси
возвращает тому кто обратился к нему. То есть, для всех кажется, что прокси такой умный классный объект, который офигенно работает, а по факту это очень тупой объект, который ничего не умеет делать, но никто про это не знает потому что за него все решает InvocationHandler, абсолютно любые методы острыми бегает его и говорит и соответственно в нашем
11:01
случае когда будет вызываться метод файл был штамм эклз или там контент и так
11:06
далее он будет выборки шимко для передавать информацию о том чтобы вызван именно этот метод параметры которые туда
11:14
пришли а дальше и напишем федор будет решать как на это реагировать и что надо вернуть

11:19 //важное!

Таким образом очень важно понимать что мы на каждый Repository-интерфейс. который мы обнаружим в нашем класспасе (а мы вообще-то делаем стартер, который все будут подключать как SpringDataJPA, и мы потом придумаем, как сделать так чтобы пользователь нашего стартера имел возможность сообщить какие пакеты надо сканировать, где искать наши репозитории, и на каждый обнаруженный репозиторий будет генерится вот это вот "Крэнг", в него будет сетиться очень умный InvocationHandler, который знает как анализировать название метода и как понять, какую логику надо запустить.

//Слайд:
PersonRepository<Person>
UserRepository<User>
TurtleRepository<Turtle>

12:05
Допустим у нас есть интерфейс, который называется PersonRepository, у нас естественно появится какой-то свой интерфейс аналогичном JPARepository у нас будет SparkRepository, при помощи дженерика мы будем сообщать, какой модели этот репозиторий. И дальше мы пропишем какие-то методы.

//Слайд "SparkInmvocationHandler долджен уметь:"

	interface PersonRepo extends SparkRepository<Person>{
	
		List<Person> findByNameContainsSortByAge(String partOfName);
		List<Person> findByAgeGreaterThan(String age);
		long.count();
		long findByAgeGreaterThanCount(String age);
		long findByAgeGreaterThanSave(String age);
		
		...
	}		

Здесь на примере сделано специально немножечко больше методов, чем это делает обычно спрингДата - кроме стандартных методов которые они поддерживают, где только фильтры и сортировка, у нас будут специфичнфе мектоды для раброты со Спарком.

12:34
# 2 про Спарк и Stream API

стоит наверное даже сейчас буквально минуточку потратить на то чтобы объяснить про spark тем кто не знаком, а для этого чуть чуть объясним концепцию стримАпи. появилась она с 8 джавы, но она появилась далеко не только там если java не добавила в себя очень много всего что связано с функциональным программированием, то наверно очень большую часть рынка потеряла, потому что для современных задач подход функционального программирования очень часто хорошо ложится на наши бизнес-задачи, особенно когда мы работаем с той же самой big data и когда уж много данных и мы не очень понимаем как нам по этим данным итерироваться, потому что они физически не влезают в одну машину, и поэтому мы хотим работать в функциональном стиле: не мы вытягиваем данные, потом по ним итерируемся и что-то с ними делаем, а мы стоим цепочку из трансформаций, передаем всякие разные функции, и потом говорим - ну давайте, кто-нибудь там запустите эту цепочку, и там вот либо Стрим АПИ, либо Spark, либо Скалавский API умеет в функциональном стиле запустить все наши объектики, все наши трансформации.

И я бы на самом деле вот такой подход сравнил бы с выпечкой пирога. У вас есть бесконечное количество трансформаций, которые вы можете сделать на ваши данные. В аллегории с пирогом данные - это какие ингредиенты, из которых мы лепим пирог. И мы там что-то отфильтровываем, потом что-то там мэпим, как-то там тесто наше сбиваем... и таких операций может быть очень много и все эти НЕ терминальные трансформационные операции всегда нам возвращают обратно нашу вот эту заготовку. Мы что-то отфильтровали, и теперь заготовка после того, как что-то отфильтровалось. Потом еще что-то отфильтровали, что-то с мэпировали, отсортировали. И в конечном итоге мы поставим пирог либо в духовку, либо мы поставим на солнышко, либо там в морозилку, либо кастрюлю чтобы сварился. То есть какая-то терминальная операция, после которой пирог будет готов и можно подавать на стол.

15:00
Мы принципе тоже можем рассматривать эти методы как какую-то цепочку действий, которые производятся на данными, а потом в конце получаем результат: у нас есть вот всякие разные findByNN..(), и у нас есть count(). В стримAPI, если я хочу в конечном итоге сохранить свои данные, то у меня нет никакой проблемы вызвать метод findByName(), он мне вернет коллекцию, а потом я эту коллекцию куда хочу туда и сохраню, и нет никакой проблемы. А когда мы говорим про Spark, то к терминальным операциям, которые поддерживаются Спринг-датой, хотелось бы еще добавить метод save().

Со Спарком сделать collect достаточно проблематичная штука, потому что если у меня данных мало collect сработает.  Но если данных много, то у меня будет out of memory эксепшен. Хоть spark он абсолютно аналогично стримAPI - там тоже есть трансформации, там даже методы все похоже называются. Но разница заключается в том что sаrk использует кластер, его можно настроить - сколько машин, сколько памяти у машин... Вы пишете по-прежнему ваши цепочки, но в отличие от stream API где максимум что можете сделать  - это сказать что наш стрим это ParallelStream, и тогда будет использоваться хотя бы многопоточность для вычисления всех этих операций. То Спарк может еще использовать очень много машин, соответственно данные размазаны по кластеру, да и в принципе данные очень часто выкачиваются из хранилища, которое тоже задействует тот же самый кластер, и наши там всякие файлы в htfs streamBucket'е. Они тоже сидят на большом количестве машин поэтому как бы их намного проще обрабатывать используя те же самые машины. И если в конце мне надо сохранить данные, то какой смысл собрать их в одну машину на драйвере, а потом как-то сохранять, может быть я захочу просто отсортировать то что мне надо, сгруппировать как надо, отфильтровать то что не надо, и сказать - а теперь сохранить.

Поэтому у меня чуть-чуть будет отличаться конвенция кроме всех этих
findBy....() у меня возможно в конце поставить какое то слово которое называется терпим как думаете по
терминальная операция так называемый финал айзен да то есть если я заканчиваю метод словом count(), это не значит что старк должен на драйвер притащить все результаты со всех машин, а потом я буду вызывать у получившегося колекшена size(), это будет сумасшедший трафик который может закончиться, зачем? Я могу сразу в методе сказать что я хочу сделать каунт того что получилось, и никто не будет эти данные мне реально вытаскивать на драйвер, или я могу добавить слово size случай если я не добавил ни какого слова будет стандартный collect ....

17:35
Вот что должно быть у нашего умного InvocationHandler'a, который будет сетится в этот прокси объект, для того чтобы он мог реализовать эту цепочку.

//Слайд "Что имеет SparkInvocationHndler?"
- засечен класс Модели
- ссылка на данные для данной модели
- DataExtractor
- Трансформации (у каждого метода свой список)
- Терминальная операция (у каждого метода своя)

Во первых в нем должен быть засечен класс модели: если мы про PersonRepository, он же назначен отвечать за Person, класс модели но хотя бы потому что модель будет хранить в себе какую-то метаинформацию о том, где находятся физически эти данные также как Hibernate: там вы пишите класс, помечаете его @Entity, а потом можете при помощи @Table() рассказать, где находится таблица. То же самое будет у меня у меня будет класс модели, и нее наверняка появится со временем какая-то аннотация @Source, которая будет ссылаться на то место, где лежит файл с этими данными. И понятно, что мой SparkInmvocationHandler, который в конечном итоге будет запускать всю цепочку трансформаций, должен будет начать с того что он вытащит эти данные, и поэтому ему естественно нужна 1) модель для того чтобы с неё считать информацию, где данные лежат,2) ему нужна модель для того чтобы знать какие есть field'ы у данной модели, потому что это тоже будет использоваться в анализах всех этих методов, 3) у нас будет DataExtractor, потому что мы за siongle responsibility, нам и так будет сложно написать красивый код, т к очень много всего сложного, поэтому мы будем стараться, чтобы наши наши объекты составлялись из вспомогательных объектов, а не  делали все сами, поэтому  соответственно у каждого InvocationHandler'a будет DataExtractor, с которым он будет начинать свою цепочку. 4) потом у него будет трансформации, которых может быть бесконечное количество. 5) И у нас будет терминальная операция
у каждого InvocationHandler'a она всегда будет одна на каждый метод.

Cколько пишу что например класс модели 1
штука навесим да конечно потому что ну если два kinder помогает песчаной почве
торе тон все методы связаны с пирсом если он и соответственно наша модель она тоже у
19:38
нас одна на весь репозитории и data extractor нас тоже один на весь репозиторий потому что неважно что будет делать
19:44
метод но если этот метод выкачивает данные то есть с там будет выкачивать данные тем же самым data extractor
19:50
потому что data extractor уже настроен выкачивать их из того места куда указала модель трансформации у нас будет много на
19:58
каждый метод методы может состоять из большого количества соваться мольбой ним и пост
20:04
бла-бла-бла and its бла-бла-бла and ордера что-то там

поэтому трансформации будет много на каждый метод, и терминальная операция будет одна, но на каждый метод.
то есть у нас уже сам видится какие-то мапы и какие-то листы...

теперь еще одна вещь должна быть SparkInvocationHandler - у него должен быть sparkSession ну или там какой-то SparkContext. Контекст - это аналог энтити-менеджера. Понятно, что если SparkInmvocationHandler будет пользоваться дата экстрактором, то для того чтобы дата экстрактор мог выкачать данные, ему нужно пользоваться SparkAPI. Сейчас не будем щас глубоко и детально уходить в SparkAPI, но основной объект который у нас в Спарке есть, он либо называется SparkSession, и с его помощью можно выкачивать данные откуда-то и потом навешивать на то что получится трансформации, либо SparkContext, если мы пользуемся более старым API. В джава это называется JavaSparkContext (они постоянно это меняют, потому что у них изначально была такая скаловская политика, что вот мы взяли какой-то класс и стали типа теперь он не интересен - либо деприкейтед, либо выпилили.... теперь надо вот этим пользоваться, он лучше). Поэтому я на всякий случай чтобы у нашего InvocationHandler'a был доступ к любому виду Spark'овских объектов тоже, я не знаю что они поменяют завтра, поэтому пусть он лучше держит референс на Контекст, в которыми естественно зарегистрированы  все бины связанные со Спарком, а он уже даже будет выковыривать что ему нужно. В нашей реализации мы будем всегда SparkSession'ом пользоваться, поэтому в принципе можно было бы его туда инжектить но так будет удобнее.

21:38

Давайте мы сразу напишем а потом будем думать что мы делаем дальше. Нам надо написать....
IDE: пустое спрингбутовое приложение
структура каталогов:

		src/main
			com/epam/repetition/
				sparkdatastarterrepetition/
					SparkDataStarterRepetitionApplication
				starter
		data
			criminals.csv
			orders.csv

Ну и нам нужен какой-то пример с данными, давайте будем о черном списке - помните такой сериал "черный список"?

	//содержимое data/criminals.csv:			
		id,name,number
		1,Ekaterina Rostova,12
		2,Edmund Dandes,13
		3,Linkoln Barrows, 43
		4,John Abruci,18

Вот так меня будет выглядеть данные. то есть это такой csv файл, в котором у нас есть три колонки: есть айдишник есть имя человека черного списка и есть его номер черном списке это не айдшник, это уникальные номера которые NAME им всем раздает.

	//содержимое data/orders.csv:		
		name,desc,price,criminalId
		Jack,kidnap,100,1
		John,kill,200,2
		Bill,murder,150,2
		Sauron,prison,1500,4

Чуть позже появится вложенная коллекция которая будет хранить заказы на всякий разный там киднэпинги, убийства, тюрьмы и так далее, которые будут по CriminalId мэпироваться, но это будет потом, когда мы дойдем до ленивых коллекций.

давайте писать это дело модели.

Раньше я когда учил как писать стартер, я всегда принципиально дело открывал 2 intelliJ, в одном писал стартер, в другом писал проект, который им пользуется. Но поскольку все таки сейчас стартер написать это не главное (хотя мы это делать будем), то это будет все в одном проекте, просто вот у меня есть отдельный пакет, который будет называться starter, и чуть позже там будут появляться все вещи которые относятся к стартеру, а тут у нас типа бизнес-логика. Эти пакеты как бы параллельны, и один другой не видит. Поэтому модель мы будем делать тут значит делаем класс модели на забавному криминал
//на трансляции всё заглючило
27:20
сделаем новый проект Spring Initializr (на java 11)
Зависимости никакие не проставляем,т к из старого проекта просто скоопируем Пом

29:22 todo рашифровать
...вот пока он это все подтягивается и синхронизируется я вам покажу зависимости очень мало у меня
зависимости на самом деле у меня есть стартовал arrow потому что мы будем
использовать hp для ленивых коллекций мы здесь имеем скалу потому что нам
некоторые вещи скале понадобится. Ну, по любому скалу за собой тянет spark, поэтому мне нужно было прописать ту версию который я хочу поэтому все равно эту библиотеку я указал.
SparkSQL мы тоже будем пользоваться. Стартер-веб принципе не нужен. но пусть будет... 
Ломбок - куда без него. 
И есть библиотека reflections, в которой есть полезные всякие штуки, поскольку мы будем писать внутри, до того как спринт построим то соответственно мне придется самому просканировать, вот эта штукой удобно это делать.
но и в принципе как у все остальное не интересно стандарт

теперь возьмем наши модел
//У автора по-прежнему глючит Идея
//32:30 отпустило

# Часть 2 
Пишем дата-класс Criminal, аннотированный @Data, @Builder и полный и пустой конструкторы.
там будут поля наших преступников - id, name, number (это не id)

Заведем для него репозиторий - интерфейс CriminalRepository extends SparkRepository<Criminal> 
(его тоже заведём).
Туда объвим метод List<Criminal> findByNumberBetween(int min,int max)

Теперь перейдем в сам стартер. В пакете starter создадим интерфейс SparkInvocationHandler extends java.lang.reflect.InvoctionHandler, и сразу имплементим  его.

Вспомним, что должно быть у нашего InvocationHandler'a: во-первых, класс самой Модели
    private Class<?> modelClass;

Во-вторых String pathToData.

В третьих - какой-то Эстрактор
    private DataExtractor dataExtractor
Сразу создадим его, это однозначно будет интерфейс,т к экстракторы у нас будут разные - для CSV, JSON, (?)паркет фйла, (?) абро файла, из таблицы...

в-четвертых, послетого как вычиталиданные,нукжноиметь лист каких-то трансфрмаций
    private List<SparkTransformation> transformationChain
Сразу заведём интерфейс SparkTransformation, их у нас тоже будет много разных.
И кстати это должен быть не лист,а Мапа, т.к. укаждогометода будет свой список трансформаций
    private Map<Method, List<SparkTransformation>> ...

И еще должна быть мапа с терминальной операцией на каждый метод
    private Map<Method, Finalizer> finalizerMap
Тоже заведем интерфейс, т.к Finalizer'ов может быть много разных - count, collect, save и т.д. 

Ну и тут еще должен сидеть весь Контекст - чтобы можно было вытащить SparkSession, или какие-то другие бины.
    private ConfigurableAppicationContext context;

38:33
Теперь давайте попытаемся записать метод Invoke(), а откуда это сюда попадет, мы потом разберемся.

На самом деле достаточно большой ценностью этого доклада может быть возможность посмотреть, как писать код. Есть много интересных техник: можно писать сверху вниз, как мы сейчас делаем.
Можно писать снизу вверх.  Можно писать через тесты, можно писать начиная со стейта.

В данном случае мне было удобней понять, что нужно иметь InvocationHandler'у для того чтобы он мог выполнять свою работу. А теперь я хочу понять, как вот эти все вещи сложатся в цепочку, при помощи которой он сделает то, что нам от него  нужно.

Первое, у нас должен быть сейчас DataExtractor, которого мы попросим принести все наши данные, значиту него должен быть такой метод типа readData(), этот метод будет принимать, откуда взять данные (pathToData), и вроде пока больше ничего, потом если что добавим..

Давайте сразу мы напишем этот метод. Он нам должен вернуть ни в коем случае не Лист, некую структуру Spark'a, потому что мы же хотим при помощи спарка делать трансформации. А когда мы вытащим данные (если это collect например), то это уже будет не spark'овский объект, а обычный лист. Но на этапе всех трансформаций мы хотим работать с спарковскими структурами. 

В Спарке есть в принципе три вида структур: есть старая структура RDD, которая очень похожа на стрим АПИ, это просто стрим объектов. У нас есть Dataset, с которым мы будем работать сегодня. Если он содержит в своем дженерике Датасет объекта(?), то это примерно как RDD, просто как стрим объектов но если он содержит в себе содержит объект Raw (типа строчка), то это будет DataFrame - то есть такая колоночная структура, и с ней очень очень удобно и эффективно работать

public interface DataExtractor {
    Dataset<Row> readData(String PathToData);
}

41:20
Идем обрато в InvocationHandlerImp. Вот мы получили наш первичный Датасет, от срочки(raw).
Dataset<Row> dataset = dataExtractor.readData(pathToData);
Теперь мне надо при помощи всех вот этих трансформаций его там как-то там фильтровать, или там что-то еще сделать. Поэтому мы можем просто проитерироваться по всем нашим <...?>

41:37
идем нашу TransformationChain и говорим, что для данного метода поэтому можем против метода получить цепочку трансформаций (хорошо что нам наш этот тупой гигант передает сюда метод который вызвали) проитерироваться по этой цепочке и попросить чтобы каждая трансформация сделала, допустим transform на этот dataset

    List<SparkTransformarion> ... = transformatioChain.get(method);
        for (SparkTransformation transformation: ...) {
            dataset.transformation.transform(dataset);
    }

Для этого у нас должен быть метод transform который принимает dataset и может быть что-то еще, но пока мы не знаем что еще. Поэтому пока оставим так всегда можно попробовать починить.

После того как мы закончили датасет трансформировать, нам надо сделать терминальную операцию, соответственно мы теперь идем в finalazerMap и говорим, что там против этого метода, отобрали финалайзер, и говорим, давай сделай файналайз. или там doAction()

	Finalizer finalizer= finalizerMap.get(method);
	finalizer.doAction(dataset);

Эта штука нам неизвестно что вернет, потому что финалайзеры могут ничего не возвращать, если они делают save(). Или  может возврщать Boolean -типа смог-не смог. Может коллекцию если это collect() может long если это был count().
Ну то что он вернет это просто вернем и мы.Назовем это retVal

	Object retVal= finalizer.doAction

И вот его-то и вернет наш InvocationHandler.

# Часть 3 

43:20
Вот мы сделали то, что должно быть на начальном этапе. И теперь, чтобы понять, что делать дальше, стоит хотя бы чуть-чуть дописать что-то из существующих классов. Например, как у нас будет работать data extractor? Давайте напишем напишем extractor для JSON файлов.

Имплементим JsonDataExtractor, и вот сразу видно чего не хватает DataExtractor'у: он захочет вытащить объект SparkSession в первую очередь. Соответственно, сюда ему надо тоже передавать этот ConfigurableApplicationContext context, из которого мы сможем вытащить бин спарксейшена, и у него есть замечательный метод read(), которому можно сказать json(..указать путь данным)
context.getBean(SparkSession.class).read().json(pathToData)
и сделать на это return;

(cигнатуру починим в интерфейсе (подозреваю что ее придется не раз чинить))
Это очень простой экстрактор, будут намного более сложные.
 
Теперь напишем другой extractor для CSV-файлов, он нам пригодится, т.к мы будем с ними работать. Копируем все, Имя CsvDataExtractor, в конце read() будет не json а csv(). Но в CSV есть намного больше нюансов, потому что в json есть схема, поэтому мне нужно будет её применить. 
Надо будет сказать через такая вот штука в ридере у Спарка - можно настроить всякие разные опции, для этого после read() ставим .option(). 

Ему мы скажем,что наш csv с колонками ("header", true), и еще скажем применить изначально схему из этих колонок. То есть ставим еще один .option("inferSchema", true), и после всех опций уже .csv()
Еще есть энкодеры, но они понадобятся потом, когда буду выкачивать этот коллекции, а пока на этом этапе они не нужны. 














